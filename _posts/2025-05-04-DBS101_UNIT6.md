---
Title: DBS101 Unit 6
categories: [DBS101, Unit6]
tags: [DBS101]
---

## --- Lesson 14 ---
### What Are Indexes?
* Purpose: Accelerate data retrieval by acting as a roadmap to locate records quickly.
* Without Indexes: Queries scan entire tables, leading to slow performance for large datasets.
* Structure: Specialized lookup tables or data structures (e.g., trees, hash tables) tied to search keys (attributes used for lookups).

### Types of Indexes
#### Ordered Indices
* Sorted Order: Entries are stored in sorted search-key order.

#### Types:
##### Clustering Index (Primary):
* Determines the physical order of records in the table (e.g., sorted by student_id).
* Only one per table (typically on the primary key).

##### Non-Clustering Index (Secondary):
* Stores search keys separately from data, with pointers to records.
* Multiple secondary indices allowed (e.g., indexing department or salary).

#### Dense vs. Sparse:
* Dense Index: Every search-key value has an entry (ideal for non-clustered data).
* Sparse Index: Only some search-key values are indexed (requires clustered data).

#### Hash Indices
* Structure: Uses a hash function to map keys to fixed-size buckets.
* Use Case: Extremely fast equality searches (e.g., WHERE id = 5).
* Limitations:
Does not support range queries (e.g., WHERE age > 30).

Bucket overflow can degrade performance.

###  Multilevel Indexing
#### Purpose: Optimize disk access for large indices by creating a hierarchy.

#### Structure:
* Outer Index: A smaller, in-memory index pointing to blocks of the inner index (stored on disk).

#### Process:
* Search the outer index to locate the relevant inner index block.
* Load the inner index block from disk.
* Use the inner index to find the data block.

### The B-Tree Family
#### B-Tree
* Structure: Self-balancing tree where each node holds multiple keys and pointers.
* Properties: All leaf nodes are at the same depth (balanced).
* Supports efficient insertions, deletions, and searches.

#### B+ Tree
##### Key Features:
* Data in Leaves: Only leaf nodes store data pointers; internal nodes guide searches.
* Linked Leaves: Leaves are linked sequentially for efficient range queries.

##### Advantages Over B-Tree:
* Higher fanout (more keys per node).
* Better suited for disk storage and large datasets.


###  Hash Indexes in Depth
#### Static Hashing:
* Fixed number of buckets.
* Risk of overflow if data grows beyond initial bucket capacity.

#### Dynamic Hashing:
* Adjusts bucket count dynamically (e.g., linear hashing).

* Minimizes overflow and avoids full rebuilds.

#### Use Cases:
* Ideal for exact-match queries (e.g., primary key lookups).
* Unsuitable for sorting or range queries.

###  Index Selection Guidelines
#### Ordered Indices (B+ Trees):
* Default choice for most databases.
* Ideal for range queries, sorting, and frequent updates.

#### Hash Indices:
* Use for high-speed equality searches (e.g., user authentication).

#### Clustering vs. Non-Clustering:
* Clustering improves range query performance but limits to one per table.
* Non-clustering allows multiple indices but adds overhead.

### Key Takeaways
##### Indexes Trade-offs: Faster queries vs. storage overhead and slower writes.
##### B+ Trees Dominate: Balanced performance for reads, writes, and range operations.
##### Hash for Speed: Optimal for point queries but inflexible for complex searches.
##### Multilevel Design: Reduces disk I/O for large datasets by layering indices.


## --- Lesson 14 ---
### 1. Query Processing Steps
* Parsing & Translation: Convert SQL to relational algebra.
* Optimization: Choose the least-cost execution plan (e.g., index usage, join order).
* Execution: Run the plan using algorithms (pipelining/materialization).

###  Cost Evaluation
#### Primary Cost Factors:
* Disk I/O: Block transfers and seeks (dominant for magnetic disks).
* Formula: Cost = (Blocks × Transfer Time) + (Seeks × Seek Time).
* SSDs: Lower seek time but higher cost for writes.

#### Response Time vs. Resource Cost: Optimizers minimize total resource usage, not response time.

### Execution Strategies
* Pipelining: Stream data between operations (reduces disk I/O).
* Materialization: Store intermediates on disk (simpler but slower).
* Iterator Model: Operators use open(), get_next(), close() for demand-driven processing.

### Core Operations
#### Selection:
* Use indexes (clustering for ranges, secondary for exact matches).
* Avoid full scans with conditions (e.g., WHERE salary > 50000).

#### Sorting:
* External Sort-Merge: Split data into sorted runs, merge chunks.
* Cost ≈ 2 × total blocks (read/write).

#### Join:
* Nested-Loop: High cost (O(n²)) but universal.
* Merge Join: Requires sorted inputs (efficient for large datasets).
* Hash Join: Fast for in-memory data.
#### Aggregation/Set Ops: Use sorting/hashing (e.g., GROUP BY, UNION).

### Optimization Techniques
* Index Smartly: Prioritize clustering indexes for frequent range queries.
* Avoid Full Scans: Use indexed nested-loop joins.
* Cache-Conscious Design: Optimize data layout for CPU cache (e.g., hash joins in cache-sized partitions).
* Query Compilation: Convert plans to machine code for faster execution.

### Key Takeaways
* B+ Trees and Hashing dominate for indexing/joins.
* Pipelining reduces I/O; Materialization simplifies complex queries.
* SSDs reduce seek time but require CPU-cost awareness.
* Focus on resource consumption (I/O, CPU) over response time for scalability.